{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presentation\n",
    "\n",
    "#### Goal: Predict the number of nights granted to a request (not to an individual)\n",
    "\n",
    "\n",
    "#### Caracteristics of the model:\n",
    "Accuracy:\n",
    "- if used as a clearing tool removing overburden upfront, and thus only to get rid of obvious cases, the accuracy of such a tool could be its most important caracteristic.\n",
    "\n",
    "Interpretability:\n",
    "- could help families understand the decision (although not as important as in diseases predictions).\n",
    "- can also highlight and thus control biases (racial, sex, age).\n",
    "- since the tool would probably be used in combination with human selection, it could help save time by highlighting the main factors for each decision\n",
    "\n",
    "#### Conclusion:\n",
    "- a model easily interpretable could be prefered (tree).\n",
    "- or a highly accurate model (less interpretable) could also be used upfront (NN).\n",
    "\n",
    "\n",
    "#### Future improvements\n",
    "\n",
    "Imputations:\n",
    "- Build more robust, generalisable imputations (eg. impute future test samples with missing gender based on all easily interpretable categories group_composition_label and group_composition_id)\n",
    "\n",
    "- Automate NaNs imputation for future test samples\n",
    "\n",
    "- Reconstruct some NaNs by training models to predict the feature\n",
    "\n",
    "\n",
    "Datasets handling\n",
    "- I made the choice to keep the train and test sets split. This is to prevent gaining insight from the test set while doing pre-processing, analysis and while training the model. To prevent duplicate code, I thus placed most of the inner workings in the class Analysis, which makes the reading less fluent. To improve, see how to better combine the visual aspect of Jupyter Notebooks, while maintaining code standards like DRY philosophy.\n",
    "\n",
    "\n",
    "Pre-processing\n",
    "- impact historical data with the known global crises (financial crisis, immigration waves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan\n",
    "\n",
    "I. Initialization\n",
    "\n",
    "1. Import packages, classes, functions\n",
    "\n",
    "2. Load databases (no join)\n",
    "\n",
    "3. Instanciate object Analysis\n",
    "\n",
    "\n",
    "II. Analyze I\n",
    "\n",
    "1. Overview\n",
    "\n",
    "2. Features: corr, dist, impact\n",
    "\n",
    "\n",
    "III. Pre-process data\n",
    "\n",
    "1. Impute NaNs\n",
    "\n",
    "2. Impute never seen before test set NaNs\n",
    "\n",
    "3. Remove outliers\n",
    "\n",
    "4. Transform categorical features\n",
    "\n",
    "5. Feature engineer\n",
    "\n",
    "\n",
    "\n",
    "IV. Analyze II\n",
    "\n",
    "1. Impact engineered features\n",
    "\n",
    "\n",
    "V. Build Model\n",
    "\n",
    "1. Benchmakrs\n",
    "\n",
    "2. Rush pytorch NN\n",
    "\n",
    "3. Simple model using principal components\n",
    "\n",
    "4. Ensemble\n",
    "\n",
    "\n",
    "VI. Predictions\n",
    "\n",
    "1. Train and Predict\n",
    "\n",
    "2. Hyperparameter tunning (split train set)\n",
    "\n",
    "\n",
    "VII. Evaluate methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages, classes, functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn import tree\n",
    "\n",
    "#os.chdir('/Users/Pro/Desktop/Git_Contests/Predictions/Emergency_housing/')\n",
    "from cobratools import Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test scorer\n",
    "def competition_scorer(y_true, y_pred):\n",
    "    return log_loss(y_true, y_pred, sample_weight=10**y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to False to only apply data transformations\n",
    "# (rather than derive the whole analysis)\n",
    "ANALYZE_ON = False\n",
    "\n",
    "# False for the competition, True if predicting > year 2019 requests\n",
    "FUTURE_PRED = False\n",
    "\n",
    "# Takes ±4min\n",
    "IMPUTE_NANS = True\n",
    "\n",
    "# Set to True to visualize\n",
    "PRINT_ON = True  # verbose\n",
    "PLOT_ON = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Join datasets?\n",
    "\n",
    "Since there are multiple requests by individuals and multiple individuals by request, the straightfoward approach would be to create columns for each individual' informations. This way, no information would be lost, but the curse of dimensionality is very near and the number of samples might be too low to extract useful information.\n",
    "\n",
    "The chosen approach is rather to only keep the request dataset' columns, and feature engineer additional columns based on the individuals data, eg.:\n",
    "- nb of past requests made by the same individual\n",
    "- nb nights granted in past requests of the same individual(s)/group\n",
    "- gender diversity of the group\n",
    "- etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests_train = pd.read_csv(filepath_or_buffer='data/requests_train.csv',\n",
    "                            sep=',',\n",
    "                            low_memory=False,\n",
    "                            error_bad_lines=False)\n",
    "\n",
    "requests_test = pd.read_csv(filepath_or_buffer='data/requests_test.csv',\n",
    "                            sep=',',\n",
    "                            low_memory=False,\n",
    "                            error_bad_lines=False)\n",
    "\n",
    "individuals_train = pd.read_csv(filepath_or_buffer='data/individuals_train.csv',\n",
    "                                sep=',',\n",
    "                                low_memory=False,\n",
    "                                error_bad_lines=False)\n",
    "\n",
    "individuals_test = pd.read_csv(filepath_or_buffer='data/individuals_test.csv',\n",
    "                            sep=',',\n",
    "                            low_memory=False,\n",
    "                            error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index col as request id\n",
    "requests_train.set_index('request_id', inplace=True)\n",
    "requests_test.set_index('request_id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instanciate object analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate analysis object with request data\n",
    "analyze_train = Analysis(requests_train)\n",
    "analyze_test = Analysis(requests_test)\n",
    "\n",
    "# Define properties\n",
    "\n",
    "# - target\n",
    "target = 'granted_number_of_nights' \n",
    "analyze_test.target = target\n",
    "analyze_train.target = target\n",
    "# - shape\n",
    "analyze_train.m = analyze_train.df.shape[0]\n",
    "analyze_test.m = analyze_test.df.shape[0]\n",
    "analyze_train.n = analyze_train.df.shape[1]\n",
    "analyze_test.n = analyze_test.df.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview\n",
    "\n",
    "- Number of requests: 238191\n",
    "\n",
    "- Number of individuals: 384133\n",
    "\n",
    "- Number of features: 39\n",
    "\n",
    "- Requests are made for 1.6 pers on average.\n",
    "\n",
    "\n",
    "Principal components\n",
    "- housing_situation_label: with value \"emergency accomodation\". High probability to get 1 or two nights. Logical since the service treats emergency housing\n",
    "- housing_situation_2_label: with value \"emergency accomodation\". High probability to get 1 or two nights.\n",
    "\n",
    "--------------------\n",
    "\n",
    "\n",
    "Analysis by features\n",
    "\n",
    "\n",
    "A. housing_situation_id\n",
    "- correlation target-housing_situation_id: -0.458581. Strong negative impact. Although the linear numerical relation of the housing_situation_id categories is in my opinion flawed, the strong correlation is explainable as the category with the smallest value \"emergency accomodation\" might be correlated with higher granted_number_of_nights than the rest of the categories, which happen to have higher housing_situation_id values.\n",
    "\n",
    "- housing_situation_2_id: 0.283840. Strong positive impact. Same explanation as housing_situation_id.\n",
    "\n",
    "B. pregnancy\n",
    "- Pregnancy seems not to have a significant direct correlation with target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANALYZE_ON:\n",
    "    analyze_train.describe(investigation_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANALYZE_ON:\n",
    "    analyze_train.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis by features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### housing_situation_2_label\n",
    "\n",
    "- ±42% (160,061) indiv with 'housing_situation_label' == 'street'\n",
    "\n",
    "- ±41% (156,496) indiv with 'housing_situation_label' == 'street' and 'housing_situation_2_label' == 'on the street'\n",
    "\n",
    "- ±75% (289,870) individuals are \"on the street\"\n",
    "\n",
    "- A majority of requests with the label \"emergency accomodation\" obtains 1 or 2 nights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANALYZE_ON:\n",
    "    # ±42% (160,061) indiv with 'housing_situation_label' == 'street'\n",
    "    df_full[df_full['housing_situation_label'] == 'street']\n",
    "\n",
    "    # ±41% (156,496) indiv with 'housing_situation_label' == 'street' and 'housing_situation_2_label' == 'on the street'\n",
    "    df_full.query(\"housing_situation_label == 'street' and housing_situation_2_label == 'on the street'\")\n",
    "\n",
    "    # ±75% (289,870) individuals are \"on the street\"\n",
    "    df_full['housing_situation_2_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANALYZE_ON:\n",
    "    #### Impact on target\n",
    "    # Impact of feature on target\n",
    "    join_key = 'request_id'\n",
    "    target = 'granted_number_of_nights'\n",
    "    feature = 'housing_situation_2_label'\n",
    "    mask = df_train[feature] == 'emergency accomodation'\n",
    "\n",
    "    # Hist: drop duplicate requests (due to indiv data merged)\n",
    "    df_train[mask][[join_key, target]].drop_duplicates().hist()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### animal_presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANALYZE_ON:\n",
    "    feature = 'animal_presence'\n",
    "    mask = df_train[feature] == 't'\n",
    "    df_train[mask][[feature, target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requester_type along with group_main_requester_id\n",
    "if it is an urgentist, used to bring individuals to the service, its groups might have higher granted rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### request_backoffice_creator_id\n",
    "this might impact since each people has its own biases (as for the predictions of court decisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process data\n",
    "\n",
    "Methodology:\n",
    "\n",
    "- Clean-up request dataset\n",
    "\n",
    "- Feature engineer using indiv dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute NaNs\n",
    "\n",
    "Methodology:\n",
    "- inspect NaNs on train set\n",
    "\n",
    "- if pattern detected, apply modifications on train and test sets\n",
    "\n",
    "Observations:\n",
    "- reverse engineering: the system seems to derive group_composition_id from group_composition_label, and both are then necessary linked/redondant => drop group_composition_label\n",
    "\n",
    "- there can be multiple individuals by request, and multiple requests by individual\n",
    "\n",
    "Further:\n",
    "- Impute 14 pregnancy NaNs from child_to_come (not useful for the current objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaNs train summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANALYZE_ON:\n",
    "    # Get Na counts: by feature, by sample\n",
    "    na_ft_train, na_sp_train = analyze_train.get_na_counts()\n",
    "    na_ft_test, na_sp_test = analyze_test.get_na_counts()\n",
    "    if PRINT_ON:\n",
    "        print('Train: NaNs count by feature\\n\\n', na_ft_train[na_ft_train!=0])\n",
    "        #print('\\n\\nTest: NaNs count by feature\\n\\n', na_ft_test[na_ft_test!=0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute child_to_come NaNs\n",
    "\n",
    "Observation\n",
    "- There are 145947 NaNs for child_to_come on the train set (in request)\n",
    "\n",
    "- There are only 14 NaNs for pregnancy in train set (in individuals)\n",
    "\n",
    "Hypotheses\n",
    "- Hyp: child_to_come is True if any indiv of the group is pregnant\n",
    "\n",
    "Conclusion\n",
    "- => Impute child_to_come from the pregnancy in the group of indiv of the request\n",
    "\n",
    "Control\n",
    "- Verify that the imputation is not only setting to 'f':\n",
    "- -> successful: from the 145947 requests, 5375 are set to 't'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANALYZE_ON:\n",
    "    # Capture the indexes where NaNs\n",
    "    df_train_raw_nas = analyze_train.df[analyze_train.df['child_to_come'].isna()]\n",
    "    idx_nas = df_train_raw_nas['child_to_come'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMPUTE_NANS:\n",
    "    # TODO: refactor (this takes ±3min)\n",
    "    # Impute train set\n",
    "    analyze_train.impute_child_to_come(df_indiv=individuals_train)\n",
    "    # Impute test set\n",
    "    analyze_test.impute_child_to_come(df_indiv=individuals_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANALYZE_ON:\n",
    "    # Get number of NaNs imputed as False and True\n",
    "    analyze_train.df.loc[idx_nas]['child_to_come'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute housing_situation_label\n",
    "\n",
    "The following code cells are organized as follows for clarity:\n",
    "- Imputation steps\n",
    "\n",
    "- Analysis\n",
    "\n",
    "Observations\n",
    "\n",
    "A. Meta numbers\n",
    "- ±7% (16,748) NaNs for housing_situation_label (request)\n",
    "\n",
    "- 0 NaN for housing_situation_id (request)\n",
    "\n",
    "- 0 NaN for housing_situation_label_2 (individuals)\n",
    "\n",
    "- 0 NaN for housing_situation_id_2 (individuals)\n",
    "\n",
    "- 21 housing_situation_label (request)\n",
    "\n",
    "- 22 housing_situation_id (request)\n",
    "\n",
    "B. Specific ratios\n",
    "- ±10% (23,309) indiv with NaN at 'housing_situation_label'\n",
    "\n",
    "- ±90% (21,185) of missing housing_situation_label are housing_situation_2_label \"on the street\" \n",
    "\n",
    "C. Analysis 1\n",
    "- housing_situation_id is derived from housing_situation_label\n",
    "\n",
    "- housing_sitaution_label NaNs have their specific id: 170\n",
    "\n",
    "- => there are actually 16748 NaNs for housing_situation_id (request)\n",
    "\n",
    "D. Analysis 2\n",
    "- housing_situation_2_label can be very diverse when housing_situation_label is NaN (17 over 21 cat)\n",
    "\n",
    "- => housing_situation_label NaNs are not produced in a specific housing_situation_2_label situation\n",
    "\n",
    "E. Analysis 3\n",
    "- all individuals of a same request share the same housing_situation_2_label\n",
    "\n",
    "- => NaNs do not come from a problem of aggregating indiv data\n",
    "\n",
    "Hypotheses\n",
    "- Hyp: the housing situation should logically have an impact on the result\n",
    "\n",
    "- Hyp: proba being on the street is significantly high to always impute NaN with 'street'\n",
    "\n",
    "- Hyp: housing_ids are not sorted in any specific order from which a logic could be derived. Thus, having numerical ids is dangereous and could lead to misinterpretations by the model.\n",
    "\n",
    "Conclusion\n",
    "- => Impute housing_situation_label NaNs as 'street'\n",
    "\n",
    "- => Drop housing_situation_id\n",
    "\n",
    "- => one-hot encode housing_situation_label\n",
    "\n",
    "\n",
    "Further improvements\n",
    "- derive more sub-groups: when a group is not housing_situation_label_2 'on the street', impute the request housing_situation_label with its most often matched value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute NaNs with the single value 'street'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMPUTE_NANS:\n",
    "    # Impute housing_situation_label NaNs as 'street'\n",
    "    analyze_train.df.loc[analyze_train.df['housing_situation_label'].isna(), 'housing_situation_label'] = 'street'\n",
    "    analyze_test.df.loc[analyze_test.df['housing_situation_label'].isna(), 'housing_situation_label'] = 'street'\n",
    "\n",
    "    # Drop housing_situation_id\n",
    "    analyze_train.df.drop('housing_situation_id', axis=1, inplace=True)\n",
    "    analyze_test.df.drop('housing_situation_id', axis=1, inplace=True)\n",
    "\n",
    "    # One-hot encoding is applied later-on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the mapping of housing_situation_id - housing_situation_label\n",
    "- housing_situation_id is derived from housing_situation_label\n",
    "\n",
    "- housing_sitaution_label NaNs have their specific id: 170\n",
    "\n",
    "- => there are actually 16748 NaNs for housing_situation_id (request)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANALYZE_ON:\n",
    "    # Get mapping housing_situation_id - housing_situation_label\n",
    "    map_housing_id_label = analyze_train.df.loc[:, ['housing_situation_id', 'housing_situation_label']].drop_duplicates()\n",
    "\n",
    "    # Sort and drop index for clarity\n",
    "    map_housing_id_label = map_housing_id_label.sort_values(by='housing_situation_id')\n",
    "    map_housing_id_label.reset_index(drop=True, inplace=True)\n",
    "    map_housing_id_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze relation between housing_situation_label (request) and housing_situation_2_label (indiv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a temporary dataframe with columns:\n",
    "\n",
    "| request_id | gr_nb_nights | hous_id | hous_lab | indiv_id | hous_2id | hous2_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANALYZE_ON:\n",
    "    # request [gr_nb_nights, hous_id, hous_lab]\n",
    "    rq = analyze_train.df.loc[:, ['granted_number_of_nights', 'housing_situation_id', 'housing_situation_label']]\n",
    "\n",
    "    # individuals [request_id, indiv_id, hous_2id, hous2_lab]\n",
    "    ind = individuals_train.loc[:, ['request_id', 'individual_id', 'housing_situation_2_id', 'housing_situation_2_label']]\n",
    "    ind.set_index('request_id', inplace=True)\n",
    "\n",
    "    # Merge request and individuals datasets\n",
    "    df_full = pd.merge(rq, ind, on='request_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANALYZE_ON:\n",
    "    # Nb of indivs (an indiv is considered a new indiv at each request)\n",
    "    nb_indivs = df_full.shape[0]\n",
    "\n",
    "    # ±6% (23,309) indiv with NaN at 'housing_situation_label'\n",
    "    df_full[df_full['housing_situation_label'].isna()]\n",
    "\n",
    "    # ±90% (21,185) of missing housing_situation_label are housing_situation_2_label \"on the street\" \n",
    "    df_temp = df_full[df_full[\"housing_situation_label\"].isna()]\n",
    "    df_temp['housing_situation_2_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect individuals with housing_situation_2_label \"on the street\":\n",
    "\n",
    "- they can have a very diverse housing_situation (17 categories over a total of 21)\n",
    "\n",
    "- => no pattern to derive from this point of view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANALYZE_ON:\n",
    "    q = df_full.query(\"housing_situation_label.isna() and housing_situation_2_label != 'on the street'\")\n",
    "\n",
    "    # Count number of categories for housing_situation_2_label\n",
    "    n_cat_2 = len(q.loc[:, 'housing_situation_2_label'].unique())\n",
    "\n",
    "    # Total categories for the feature\n",
    "    n_cat_1 = len(analyze_train.get_col_uniques('housing_situation_label'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if housing_situation_label NA exist when indiv within a group have different housing_situation_2\n",
    "\n",
    "- no divergence\n",
    "\n",
    "- => when group, all indiv have the same housing_situation_2_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANALYZE_ON:\n",
    "    # takes ±1min\n",
    "    if False:\n",
    "        # Get request_id along with its group size\n",
    "        rq_id = df_full.index.value_counts()\n",
    "        for i in range(70000):\n",
    "            # Get ith rq_id\n",
    "            rq_id_i = rq_id.index[0]\n",
    "\n",
    "            # Observe if housing_situation_label same for all the group members\n",
    "            n_uni_grp = len(df_full.loc[rq_id_i]['housing_situation_label'].unique())\n",
    "            n_uni_indiv = len(df_full.loc[rq_id_i]['housing_situation_2_label'].unique())\n",
    "\n",
    "            if n_uni_grp > 1 or n_uni_indiv > 1:\n",
    "                print(i)\n",
    "                print(n_uni_grp, n_uni_indiv)\n",
    "                print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further improvements: derive the most probable mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANALYZE_ON:\n",
    "    # Get mapping housing_situation_label - housing_situation_2_label\n",
    "    df_full_no_na = df_full[~df_full.housing_situation_label.isna()]\n",
    "    map_housing_labels = df_full_no_na.loc[:, ['housing_situation_label', 'housing_situation_2_label']].drop_duplicates()\n",
    "\n",
    "    # Sort and drop index for clarity\n",
    "    map_housing_labels = map_housing_labels.sort_values(by='housing_situation_label')\n",
    "    map_housing_labels.reset_index(drop=True, inplace=True)\n",
    "    map_housing_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute long_term_housing_request NaNs\n",
    "\n",
    "Nb NaNs: 165556"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute town NaNs\n",
    "\n",
    "Nb NaNs: 159959"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute victim_of_violence_type NaNs\n",
    "\n",
    "Nb NaNs: 234175\n",
    "\n",
    "is NaN if victim_of_violence is 'f'\n",
    "\n",
    "\n",
    "=> Replace these NaNs by say 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporarily drop columns with NaNs remaining\n",
    "(untill imputation methods are implemented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: impute properly\n",
    "list_drop = [\n",
    "    'long_term_housing_request',\n",
    "    'town',\n",
    "    'victim_of_violence_type'\n",
    "]\n",
    "for col in list_drop:\n",
    "    analyze_train.df.drop(col, axis=1, inplace=True)\n",
    "    analyze_test.df.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporarily drop numerical columns\n",
    "(that can't be used properly untill transformation methods are implemented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: transform rather than drop\n",
    "list_drop = [\n",
    "    'child_situation',\n",
    "    'district',\n",
    "    'group_composition_id',\n",
    "    'group_id',\n",
    "    'group_main_requester_id',\n",
    "    'request_backoffice_creator_id',\n",
    "    'social_situation_id'\n",
    "]\n",
    "\n",
    "for feature in list_drop:\n",
    "    analyze_train.df.drop(feature, axis=1, inplace=True)\n",
    "    analyze_test.df.drop(feature, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute remaining test set NaNs\n",
    "(that have no equivalent in train set, and thus can't be studied to build a clever imputation method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement method based on train set logic for any feature\n",
    "analyze_train.set_default_na_vals()\n",
    "\n",
    "# Transfer default NaNs to test object\n",
    "analyze_test.default_na_vals = analyze_train.default_na_vals\n",
    "\n",
    "# Impute any remaining NaN based on its default value\n",
    "analyze_test.impute_nans()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop answer creation date\n",
    "hyp: the variable is not available at prediction time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the purpose of the competition, is it expected to be used?\n",
    "if FUTURE_PRED:\n",
    "    analyze_train.df.drop('answer_creation_date', axis=1, inplace=True)\n",
    "    analyze_test.df.drop('answer_creation_date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete old samples\n",
    "(if it was to predict future requests (> 2020))\n",
    "- Train/test split being done randomly (≠ historically), it is important for this competition to train the model on the whole train set (don't remove old samples)\n",
    "- Delete samples with group_creation_date < 2015, since it is very unlikely that current demands are treated like +5 years ago (social services evolve)\n",
    "- Threshold date: see if later is better, potential gains from domain knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FUTURE_PRED:\n",
    "    # Drop samples with year < 2015\n",
    "    old_samples = analyze_train.df[analyze_train.df.group_creation_date.dt.year < 2015]\n",
    "    analyze_train.df.drop(old_samples.index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender\n",
    "\n",
    "- => Only females are possibly pregnant, thus 30 males have made a mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Retrieve the individual ids, and correct for male -> pregnancy = 0\n",
    "#analyze_train.df['gender'].groupby(analyze_train.df['pregnancy']).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusterize large categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make clusters then transform using one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform dates\n",
    "- into linear numerical features (year, month)\n",
    "\n",
    "- and into categorical features (hot_season, col_season:T/F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features: Dates to year, month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns of date type\n",
    "list_date_cols = [\n",
    "    'request_creation_date',\n",
    "    'group_creation_date'\n",
    "]\n",
    "\n",
    "# Don't use the feature if trying to build robust in-production model\n",
    "if not FUTURE_PRED:\n",
    "    list_date_cols.append('answer_creation_date')\n",
    "\n",
    "# Transform date type: string to timestamp\n",
    "for col in list_date_cols:\n",
    "    analyze_train.df[col] = pd.to_datetime(analyze_train.df[col])\n",
    "    analyze_test.df[col] = pd.to_datetime(analyze_test.df[col])\n",
    "\n",
    "# Create feature: 'year'\n",
    "for col in list_date_cols:\n",
    "    analyze_train.df[col[:-4]+'year'] = analyze_train.df[col].dt.year\n",
    "    analyze_test.df[col[:-4]+'year'] = analyze_test.df[col].dt.year\n",
    "\n",
    "# Create feature: 'month'\n",
    "for col in list_date_cols:\n",
    "    analyze_train.df[col[:-4]+'month'] = analyze_train.df[col].dt.month\n",
    "    analyze_test.df[col[:-4]+'month'] = analyze_test.df[col].dt.month\n",
    "\n",
    "# Drop raw features of type date\n",
    "for col in list_date_cols:\n",
    "    analyze_train.df.drop(col, axis=1, inplace=True)\n",
    "    analyze_test.df.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Feats: hot_season, col_season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "hot_months = [7, 8]\n",
    "col_months = [1, 1, 11, 12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Town to regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Feat: nb of indivs in the group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Feat: nb of past requests by indivs forming the group of the request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# Single individual\n",
    "\n",
    "# Get number of ALL requests of indiv with max n_requests\n",
    "#ind_id = df_full['individual_id'].value_counts().index[0]\n",
    "\n",
    "# Watch request made by the same indiv\n",
    "#df_full[df_full['individual_id'] == ind_id]\n",
    "\n",
    "# TODO: Past requests only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Feat: nb of past granted request by indivs forming the group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform categorical features\n",
    "Prepare data to feed models\n",
    "\n",
    "- booleans: replace by (1, 0)\n",
    "\n",
    "- 2 < cats < 11: one-hot encoding\n",
    "\n",
    "- No transform on features with more than 11 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ANALYZE_ON:\n",
    "    # Display col name along its type\n",
    "    for col, col_type in zip(analyze_train.df.columns, analyze_train.get_cols_type()):\n",
    "        print(col_type, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform all categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Transform Boolean at col animal_presence: True/False=['f' 't']\nTransform Boolean at col child_to_come: True/False=[False  True]\n\nERROR - Transform Boolean at col group_type not recognized: ['individual' 'group']\n\nTransform Boolean at col victim_of_violence: True/False=['f' 't']\nTransform Boolean at col animal_presence: True/False=['f' 't']\nTransform Boolean at col child_to_come: True/False=[False  True]\n\nERROR - Transform Boolean at col group_type not recognized: ['group' 'individual']\n\nTransform Boolean at col victim_of_violence: True/False=['f' 't']\n\nERROR - Transform Boolean at col answer_creation_year not recognized: [2019 2018]\n\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(('group', 'individual'), False)"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# Pre-process columns:\n",
    "# - booleans: 't', 't' => True, False\n",
    "# - Categorical with few classes => one-hot encoding\n",
    "bools_train, failed_train = analyze_train.transform_categories(target=analyze_train.target)\n",
    "bools_test, failed_test = analyze_test.transform_categories(target=analyze_test.target)\n",
    "\n",
    "# Preprocess specific cat columns\n",
    "analyze_train.convert_to_bool(col='group_type',\n",
    "                            true_val='group',\n",
    "                            false_val='individual')\n",
    "\n",
    "# Preprocess specific cat columns\n",
    "analyze_test.convert_to_bool(col='group_type',\n",
    "                            true_val='group',\n",
    "                            false_val='individual')\n",
    "\n",
    "# Export data\n",
    "#analyze_train.export_data('data/data_train_preprocessed.csv')\n",
    "#analyze_test.export_data('data/data_test_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis II\n",
    "(df_train)\n",
    "\n",
    "Analysis focused on impact of one-hot encoded variables and new engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dates\n",
    "Link btw group_creation_date and request_creation_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "#pd.DataFrame([rq_crea_dt_seconds, grp_crea_dt_seconds]).corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random uniform train/test\n",
    "random_preds_train = np.random.uniform(size=(requests_train.shape[0], 4))\n",
    "random_preds_test = np.random.uniform(size=(requests_test.shape[0], 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import Linear\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ma_cols_bool = [False] * 12 + [True] * (len(analyze_train.df.columns) - 12)\n",
    "#analyze_test.df.loc[:,ma_cols_bool] = analyze_test.df.loc[:,ma_cols_bool].astype(np.uint8)\n",
    "#analyze_train.df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set train/cross-validation split\n",
    "TRAIN_VAL_SPLIT = 0.7\n",
    "n_train_samples = round(analyze_test.df.shape[0] * TRAIN_VAL_SPLIT)\n",
    "\n",
    "# Mask - all features columns but target\n",
    "ma_feats = analyze_train.df.columns != target\n",
    "\n",
    "# Split train and cross-val sets && features (X) vs target (Y)\n",
    "X_train = analyze_train.df.iloc[n_train_samples:, ma_feats]\n",
    "X_val = analyze_train.df.iloc[:n_train_samples, ma_feats]\n",
    "Y_train = analyze_train.df[target][n_train_samples:]\n",
    "Y_val = analyze_train.df[target][:n_train_samples]\n",
    "\n",
    "# Transform sets to torch tensors\n",
    "X_train = torch.from_numpy(X_train.values)\n",
    "X_val = torch.from_numpy(X_val.values)\n",
    "Y_train = torch.from_numpy(Y_train.values)\n",
    "Y_val = torch.from_numpy(Y_val.values)\n",
    "\n",
    "# Split X-Y and transform to tensors\n",
    "X_test = torch.from_numpy(analyze_test.df.loc[:, ma_feats].values)\n",
    "Y_test = torch.from_numpy(analyze_test.df[target].values)\n",
    "\n",
    "# Cast to float type\n",
    "X_train = X_train.float()\n",
    "X_val = X_val.float()\n",
    "Y_train = Y_train.float()\n",
    "Y_val = Y_val.float()\n",
    "X_test = X_test.float()\n",
    "Y_test = Y_test.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, X, Y):\n",
    "        # Cast Y to long for CrossEntropyLoss\n",
    "        self.Y = Y.type(torch.LongTensor)\n",
    "        self.X = X\n",
    "        self.len=len(self.X)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x=self.X[idx]\n",
    "        y=self.Y[idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate datasets\n",
    "dataset_train = Dataset(X=X_train, Y=Y_train)\n",
    "dataset_val = Dataset(X=X_val, Y=Y_val)\n",
    "dataset_test = Dataset(X=X_test, Y=Y_test)\n",
    "\n",
    "# Instanciate data loaders\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size = 100000)\n",
    "dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size = 100000)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size = 100000)\n",
    "\n",
    "# Set log-loss criterion\n",
    "y_vals = [0, 1, 2, 3]\n",
    "weights = [10**y for y in y_vals]\n",
    "class_weights = torch.FloatTensor(weights)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "\n",
    "        for input_size, output_size in zip(layers, layers[1:]):\n",
    "            self.hidden_layers.append(nn.Linear(input_size, output_size))\n",
    "\n",
    "    def forward(self, activation):\n",
    "        for i_layer, linear_transform in enumerate(self.hidden_layers):\n",
    "            if i_layer < len(self.hidden_layers) - 1:\n",
    "                activation = torch.relu(linear_transform(activation))\n",
    "            else:\n",
    "                activation = linear_transform(activation)\n",
    "        return activation\n",
    "\n",
    "layers = [analyze_train.df.columns.size-1, 35, 25, 10, 4]\n",
    "model = Net(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize weights\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.3, momentum=0.1)\n",
    "\n",
    "# Reset loss history\n",
    "loss_epochs = {\n",
    "    'train': [],\n",
    "    'eval': []\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch 0, batch 0, loss 11.5708\nepoch 0, batch 1, loss 34345108.0\nAvg epoch loss train: 17172559.7854 - eval: 1.4006\n\nepoch 1, batch 0, loss 1.4015\nepoch 1, batch 1, loss 1.339\nAvg epoch loss train: 1.3702 - eval: 1.2788\n\nepoch 2, batch 0, loss 1.2805\nepoch 2, batch 1, loss 1.2344\nAvg epoch loss train: 1.2574 - eval: 1.1925\n\nepoch 3, batch 0, loss 1.1947\nepoch 3, batch 1, loss 1.1636\nAvg epoch loss train: 1.1791 - eval: 1.1365\n\nepoch 4, batch 0, loss 1.1395\nepoch 4, batch 1, loss 1.1191\nAvg epoch loss train: 1.1293 - eval: 1.0988\n\n"
    }
   ],
   "source": [
    "# Set seed to reproduce results\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Train model\n",
    "n_epochs = 50\n",
    "\n",
    "# Loop over epochs\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # Lists batchs loss\n",
    "    loss_batchs = {\n",
    "        'train': [],\n",
    "        'eval': []\n",
    "        }\n",
    "\n",
    "    # Training\n",
    "    # --------\n",
    "    for n_batch, (x, labels) in enumerate(dataloader_train):\n",
    "        \n",
    "        # Predict\n",
    "        predictions = model(x)\n",
    "        \n",
    "        # Evaluate\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # Compute gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Backpropagate\n",
    "        optimizer.step()\n",
    "\n",
    "        # Add batch loss to the list of current epoch\n",
    "        loss_batchs['train'].append(round(loss.item(), 4))\n",
    "\n",
    "        print('epoch {}, batch {}, loss {}'.format(epoch, n_batch, round(loss.item(), 4)))\n",
    "\n",
    "\n",
    "    # Validation\n",
    "    # ----------\n",
    "    for n_batch, (x, labels) in enumerate(dataloader_val):\n",
    "        # Predict on the eval set\n",
    "        predictions = model(x)\n",
    "        \n",
    "        # Compute batch loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # Add batch loss to the list of current epoch\n",
    "        loss_batchs['eval'].append(round(loss.item(), 4))\n",
    "\n",
    "\n",
    "\n",
    "    # Compute train and eval losses for the epoch, and add to epochs list\n",
    "    for set_name in ['train', 'eval']:\n",
    "        loss_epoch = torch.tensor(loss_batchs[set_name], dtype=float).mean()\n",
    "        loss_epochs[set_name].append(round(loss_epoch.item(), 4))\n",
    "\n",
    "    \n",
    "    print(f\"Avg epoch loss train: {loss_epochs['train'][-1]} - eval: {loss_epochs['eval'][-1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"259.116562pt\" version=\"1.1\" viewBox=\"0 0 378.465625 259.116562\" width=\"378.465625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 259.116562 \nL 378.465625 259.116562 \nL 378.465625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 36.465625 235.238437 \nL 371.265625 235.238437 \nL 371.265625 17.798437 \nL 36.465625 17.798437 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mca4b380115\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.683807\" xlink:href=\"#mca4b380115\" y=\"235.238437\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0.0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(43.732244 249.836875)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"89.729261\" xlink:href=\"#mca4b380115\" y=\"235.238437\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 0.5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(81.777699 249.836875)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"127.774716\" xlink:href=\"#mca4b380115\" y=\"235.238437\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 1.0 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(119.823153 249.836875)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"165.82017\" xlink:href=\"#mca4b380115\" y=\"235.238437\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 1.5 -->\n      <g transform=\"translate(157.868608 249.836875)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"203.865625\" xlink:href=\"#mca4b380115\" y=\"235.238437\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 2.0 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(195.914063 249.836875)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"241.91108\" xlink:href=\"#mca4b380115\" y=\"235.238437\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 2.5 -->\n      <g transform=\"translate(233.959517 249.836875)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"279.956534\" xlink:href=\"#mca4b380115\" y=\"235.238437\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 3.0 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(272.004972 249.836875)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"318.001989\" xlink:href=\"#mca4b380115\" y=\"235.238437\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 3.5 -->\n      <g transform=\"translate(310.050426 249.836875)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"356.047443\" xlink:href=\"#mca4b380115\" y=\"235.238437\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 4.0 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(348.095881 249.836875)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m18ce4622cc\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m18ce4622cc\" y=\"225.354814\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.00 -->\n      <g transform=\"translate(7.2 229.154033)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m18ce4622cc\" y=\"196.577401\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.25 -->\n      <g transform=\"translate(7.2 200.376619)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m18ce4622cc\" y=\"167.799987\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.50 -->\n      <g transform=\"translate(7.2 171.599206)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m18ce4622cc\" y=\"139.022574\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.75 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g transform=\"translate(7.2 142.821793)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m18ce4622cc\" y=\"110.245161\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 1.00 -->\n      <g transform=\"translate(7.2 114.044379)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m18ce4622cc\" y=\"81.467747\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 1.25 -->\n      <g transform=\"translate(7.2 85.266966)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m18ce4622cc\" y=\"52.690334\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 1.50 -->\n      <g transform=\"translate(7.2 56.489553)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m18ce4622cc\" y=\"23.912921\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 1.75 -->\n      <g transform=\"translate(7.2 27.71214)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_18\">\n     <!-- 1e7 -->\n     <defs>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n     </defs>\n     <g transform=\"translate(36.465625 14.798437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-49\"/>\n      <use x=\"63.623047\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"125.146484\" xlink:href=\"#DejaVuSans-55\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#p1fb76d3d40)\" d=\"M 51.683807 27.682074 \nL 127.774716 225.354798 \nL 203.865625 225.354799 \nL 279.956534 225.3548 \nL 356.047443 225.354801 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path clip-path=\"url(#p1fb76d3d40)\" d=\"M 51.683807 225.354798 \nL 127.774716 225.354799 \nL 203.865625 225.3548 \nL 279.956534 225.354801 \nL 356.047443 225.354801 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 36.465625 235.238437 \nL 36.465625 17.798438 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 371.265625 235.238437 \nL 371.265625 17.798438 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 36.465625 235.238437 \nL 371.265625 235.238437 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 36.465625 17.798437 \nL 371.265625 17.798437 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p1fb76d3d40\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"36.465625\" y=\"17.798437\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.plot(loss_epochs['train'])\n",
    "plt.plot(loss_epochs['eval'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Avg loss test: 1.1012\n\n"
    }
   ],
   "source": [
    "# Lists batchs loss\n",
    "loss_batchs = {\n",
    "    'test': []\n",
    "    }\n",
    "\n",
    "# Test set\n",
    "# --------\n",
    "for n_batch, (x, labels) in enumerate(dataloader_test):\n",
    "    # Predict on the test set\n",
    "    predictions = model(x)\n",
    "    \n",
    "    # Compute batch loss\n",
    "    loss = criterion(predictions, labels)\n",
    "    \n",
    "    # Add batch loss to the list of current epoch\n",
    "    loss_batchs['test'].append(round(loss.item(), 4))\n",
    "\n",
    "\n",
    "# Compute train and test losses for the epoch, and add to epochs list\n",
    "loss_test = torch.tensor(loss_batchs['test'], dtype=float).mean()\n",
    "loss_test = round(loss_test.item(), 4)\n",
    "\n",
    "print(f\"Avg loss test: {loss_test}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observed a significant (negative) correlation of housing_situation_id with granted_number_of_nights, let's train a univariate model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model' parameters\n",
    "clf = tree.DecisionTreeClassifier(\n",
    "    criterion='gini',\n",
    "    splitter='best',\n",
    "    max_depth=2,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features=None,\n",
    "    max_leaf_nodes=None,\n",
    "    class_weight=None)\n",
    "\n",
    "# Build train/test datasets with housing_situation_id only\n",
    "X_train = np.array(requests_train['housing_situation_id']).reshape(-1, 1)\n",
    "X_test = np.array(requests_test['housing_situation_id'].values).reshape(-1, 1)\n",
    "Y_train = requests_train.granted_number_of_nights.values\n",
    "Y_test = requests_test.granted_number_of_nights.values\n",
    "\n",
    "# Transform categorical target into a one-hot vector\n",
    "Y_train_onehot = to_onehot(Y_train)\n",
    "Y_test_onehot = to_onehot(Y_test)\n",
    "\n",
    "# Train the model\n",
    "clf = clf.fit(X_train, Y_train_onehot)\n",
    "\n",
    "# Yield train/test predictions\n",
    "preds_train_tree_univar = clf.predict(X_train)\n",
    "preds_test_tree_univar = clf.predict(X_test)\n",
    "\n",
    "# Fill predictions to .2 elsewhere\n",
    "preds_train_tree_univar[preds_train_tree_univar == 0] = .2\n",
    "preds_test_tree_univar[preds_test_tree_univar == 0] = .2\n",
    "\n",
    "# Evaluate train/test\n",
    "score_train = competition_scorer(Y_train, preds_train_tree_univar)\n",
    "score_test = competition_scorer(Y_test, preds_test_tree_univar)\n",
    "\n",
    "# Display results\n",
    "print(f'train score: {score_train:.2f}')\n",
    "print(f'test score: {score_test:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = clf.predict_proba(X_train)\n",
    "v0 = probas[0].max(1)\n",
    "v1 = probas[1].max(1)\n",
    "v2 = probas[2].max(1)\n",
    "v3 = probas[3].max(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_test = requests_test.granted_number_of_nights.values\n",
    "\n",
    "# Evaluate benchmarks\n",
    "random_score_test = competition_scorer(y_true_test, random_preds_test)\n",
    "dumb_score_test = competition_scorer(y_true_test, dumb_preds_test)\n",
    "\n",
    "# Display results\n",
    "print(f'test score random: {random_score_test:.2f}')\n",
    "print(f'test score dumb: {dumb_score_test:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_train = requests_train.granted_number_of_nights.values\n",
    "\n",
    "# Evaluate benchmarks\n",
    "random_score_train = competition_scorer(y_true_train, random_preds_train)\n",
    "dumb_score_train = competition_scorer(y_true_train, dumb_preds_train)\n",
    "\n",
    "# Display results\n",
    "print(f'train score random: {random_score_train:.2f}')\n",
    "print(f'train score dumb: {dumb_score_train:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree\n",
    "fn = ['housing_situation_id']\n",
    "cn = ['0', '1', '2', '3']\n",
    "\n",
    "tree.plot_tree(clf,\n",
    "               feature_names = fn, \n",
    "               class_names=cn,\n",
    "               filled = True)\n",
    "\n",
    "requests_train['housing_situation_id'].hist()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36264bitpytorchvirtualenvb2c3f00e98dc48dba8e2131dc30ce6c5",
   "display_name": "Python 3.6.2 64-bit ('pytorch': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}